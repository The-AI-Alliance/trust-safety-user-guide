---
layout: default
title: Exploring AI Trust and Safety
nav_order: 40
has_children: true
---

# Exploring AI Trust and Safety

What is required for us to trust AI? There are many ways to understand and approach risk. Let’s review a few ways of categorizing risk and mitigation strategies that experts in the field have developed, many of whom are AI Alliance members. We will start with some broad perspectives on risk management, then explore more detailed taxonomies of risk and mitigation tools. Finally, we offer specific information about [Cybersecurity]({{site.glossaryurl}}/#cybersecurity) in AI Systems.

* [NIST Artificial Intelligence Risk Management Framework]({{site.baseurl}}/exploring/nist-risk-framework): A framework developed by the National Institute of Standards and Technology, under the United States Department of Commerce.
* [Trust and Safety at Meta]({{site.baseurl}}/exploring/meta-trust-safety): R&D at Meta on trust and safety.
* [Mozilla Foundation's guidance on Trustworthy AI]({{site.baseurl}}/exploring/mozilla-trustworthy-ai): Mozilla Foundation's guidance on ensuring trustworthy AI.
* [MLCommons AILuminate]({{site.baseurl}}/exploring/mlcommons-ailuminate): The influential risk taxonomy and corresponding benchmarks from the MLCommons industry collaboration.
* [The Trusted AI (TAI) Frameworks Project]({{site.baseurl}}/exploring/tai-frameworks): Trustworthiness research from an academic and armed forces collaboration.
* [Cybersecurity]({{site.baseurl}}/exploring/cybersecurity): New security considerations when using AI.
	
Here are some additional sources of information that we won't cover in more detail here, although we may expand coverage of them at a later date. See also the [References]({{site.baseurl}}/references):

* [International AI Safety Report 2025](https://www.gov.uk/government/publications/international-ai-safety-report-2025){:target="iaisr-2025"}: A report on the state of advanced AI capabilities and risks – written by 100 AI experts including representatives nominated by 33 countries and intergovernmental organizations (January 29, 2025).
* ACM Public Policy Products, _Comments in Response to European Commission Call for Evidence Survey on “Artificial Intelligence - Implementing Regulation Establishing a Scientific Panel of Independent Experts”_ [PDF](https://www.acm.org/binaries/content/assets/public-policy/acm-europetpc-consultation-2024---general-purpose-ai-code-of-practice.pdf){:target="acm-europe"}: published by the ACM Europe Technology Policy Committee (November 15, 2024).
* [The AI inflection point](https://www.adobe.com/acrobat/business/reports/sdk/ai-inflection-point.html){:target="adobe"}: Adobe's recommendations for responsible AI in organizations (published December 2024).
* [ClairBot](https://clair.bot/){:target="clairbot"} from the Responsible AI Team at [Ekimetrics](https://ekimetrics.com/){:target="ekimetrics"} is a research project that compares several model responses for domain-specific questions, where each of the models has been tuned for a particular domain, in this case ad serving, laws and regulations, and social sciencies and ethics.

See also the [References]({{site.baseurl}}/references).

---

After reviewing the exploration here, consider our specific [recommendations]({{site.baseurl}}/safety-recommendations/safety-recommendations) for successful, safe use of AI.
