---
layout: default
title: Exploring AI Trust and Safety
nav_order: 40
has_children: true
---

# Exploring AI Trust and Safety

What is required for us to trust AI? There are many ways to understand and approach risk. Letâ€™s review a few ways of categorizing risk and mitigation strategies that experts in the field have developed, many of whom are AI Alliance members. We will start with some broad perspectives on risk management, then explore more detailed taxonomies of risk and mitigation tools.

* [NIST Artificial Intelligence Risk Management Framework]({{site.baseurl}}/exploring/nist-risk-framework): A framework developed by the National Institute of Standards and Technology, under the United States Department of Commerce.
* [Trust and Safety at Meta]({{site.baseurl}}/exploring/meta-trust-safety).
* [Mozilla Foundation's guidance on Trustworthy AI]({{site.baseurl}}/exploring/mozilla-trustworthy-ai).
* [MLCommons Taxonomy of Hazards]({{site.baseurl}}/exploring/mlcommons-taxonomy-hazards).
* [The Trusted AI (TAI) Frameworks Project]({{site.baseurl}}/exploring/tai-frameworks).

Next we provide some specific [recommends]({{site.baseurl}}/safety-recommendations/safety-recommendations) for successful, safe use of AI.